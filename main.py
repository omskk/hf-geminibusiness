import json, time, hmac, hashlib, base64, os, asyncio, uuid, ssl, re
from datetime import datetime
from typing import List, Optional, Union, Dict, Any
import logging
from contextlib import asynccontextmanager

import httpx
from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

from dotenv import load_dotenv
# Load .env file first
load_dotenv()

from database import db  # Import database instance

# ---------- æ—¥å¿—é…ç½® ----------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s | %(levelname)s | %(message)s",
    datefmt="%H:%M:%S",
)
logger = logging.getLogger("gemini")

# ---------- é…ç½® ----------
API_KEY      = os.getenv("API_KEY")
# Fallsback ENV variables (customary support)
ENV_SECURE_C_SES = os.getenv("SECURE_C_SES")
ENV_HOST_C_OSES  = os.getenv("HOST_C_OSES")
ENV_CSESIDX      = os.getenv("CSESIDX")
ENV_CONFIG_ID    = os.getenv("CONFIG_ID")

PROXY        = os.getenv("PROXY") or None
TIMEOUT_SECONDS = 600 

# ---------- æ¨¡å‹æ˜ å°„é…ç½® ----------
MODEL_MAPPING = {
    "gemini-auto": None,
    "gemini-2.5-flash": "gemini-2.5-flash",
    "gemini-2.5-pro": "gemini-2.5-pro",
    "gemini-3-flash-preview": "gemini-3-flash-preview",
    "gemini-3-pro-preview": "gemini-3-pro-preview"
}

# ---------- HTTP å®¢æˆ·ç«¯ ----------
http_client = httpx.AsyncClient(
    proxy=PROXY,
    verify=False,
    http2=False,
    timeout=httpx.Timeout(TIMEOUT_SECONDS, connect=60.0),
    limits=httpx.Limits(max_keepalive_connections=20, max_connections=50)
)

# ---------- å·¥å…·å‡½æ•° ----------
def get_common_headers(jwt: str) -> dict:
    return {
        "accept": "*/*",
        "accept-encoding": "gzip, deflate, br, zstd",
        "accept-language": "zh-CN,zh;q=0.9,en;q=0.8",
        "authorization": f"Bearer {jwt}",
        "content-type": "application/json",
        "origin": "https://business.gemini.google",
        "referer": "https://business.gemini.google/",
        "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36",
        "x-server-timeout": "1800",
        "sec-ch-ua": '"Chromium";v="124", "Google Chrome";v="124", "Not-A.Brand";v="99"',
        "sec-ch-ua-mobile": "?0",
        "sec-ch-ua-platform": '"Windows"',
        "sec-fetch-dest": "empty",
        "sec-fetch-mode": "cors",
        "sec-fetch-site": "cross-site",
    }

def urlsafe_b64encode(data: bytes) -> str:
    return base64.urlsafe_b64encode(data).decode().rstrip("=")

def kq_encode(s: str) -> str:
    b = bytearray()
    for ch in s:
        v = ord(ch)
        if v > 255:
            b.append(v & 255)
            b.append(v >> 8)
        else:
            b.append(v)
    return urlsafe_b64encode(bytes(b))

def create_jwt(key_bytes: bytes, key_id: str, csesidx: str) -> str:
    now = int(time.time())
    header = {"alg": "HS256", "typ": "JWT", "kid": key_id}
    payload = {
        "iss": "https://business.gemini.google",
        "aud": "https://biz-discoveryengine.googleapis.com",
        "sub": f"csesidx/{csesidx}",
        "iat": now,
        "exp": now + 300,
        "nbf": now,
    }
    header_b64  = kq_encode(json.dumps(header, separators=(",", ":")))
    payload_b64 = kq_encode(json.dumps(payload, separators=(",", ":")))
    message     = f"{header_b64}.{payload_b64}"
    sig         = hmac.new(key_bytes, message.encode(), hashlib.sha256).digest()
    return f"{message}.{urlsafe_b64encode(sig)}"

# ---------- JWT ç®¡ç† (Per Account) ----------
class JWTManager:
    def __init__(self, account_data: dict) -> None:
        self.account = account_data
        self.jwt: str = ""
        self.expires: float = 0
        self._lock = asyncio.Lock()

    async def get(self) -> str:
        async with self._lock:
            if time.time() > self.expires:
                await self._refresh()
            return self.jwt

    async def _refresh(self) -> None:
        cookie = f"__Secure-C_SES={self.account['secure_c_ses']}"
        if self.account.get('host_c_oses'):
            cookie += f"; __Host-C_OSES={self.account['host_c_oses']}"
        
        logger.debug(f"ğŸ”‘ [{self.account['id']}] æ­£åœ¨åˆ·æ–° JWT...")
        try:
            r = await http_client.get(
                "https://business.gemini.google/auth/getoxsrf",
                params={"csesidx": self.account['csesidx']},
                headers={
                    "cookie": cookie,
                    "user-agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/140.0.0.0 Safari/537.36",
                    "referer": "https://business.gemini.google/"
                },
            )
            if r.status_code != 200:
                logger.error(f"âŒ [{self.account['id']}] getoxsrf å¤±è´¥: {r.status_code} {r.text}")
                raise HTTPException(r.status_code, "getoxsrf failed")
            
            txt = r.text[4:] if r.text.startswith(")]}'") else r.text
            data = json.loads(txt)

            key_bytes = base64.urlsafe_b64decode(data["xsrfToken"] + "==")
            self.jwt     = create_jwt(key_bytes, data["keyId"], self.account['csesidx'])
            self.expires = time.time() + 270
            logger.info(f"âœ… [{self.account['id']}] JWT åˆ·æ–°æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ [{self.account['id']}] JWT Refresh Error: {e}")
            raise e

# ---------- è´¦å·ä¸ä¼šè¯ç®¡ç† ----------
class Account:
    def __init__(self, data: dict):
        self.id = data.get("id") or 0
        self.name = data.get("name") or f"Account-{self.id}"
        self.secure_c_ses = data["secure_c_ses"]
        self.host_c_oses = data.get("host_c_oses")
        self.csesidx = data["csesidx"]
        self.config_id = data["config_id"]
        
        self.jwt_mgr = JWTManager(data)
        self.lock = asyncio.Lock() # For account-level operations if needed

    async def get_jwt(self):
        return await self.jwt_mgr.get()

class AccountPool:
    def __init__(self):
        self.accounts: List[Account] = []
        self._current_index = 0
        self._lock = asyncio.Lock()

    async def load_accounts(self):
        try:
            await db.connect()
            rows = await db.fetch_active_accounts()
            if rows:
                self.accounts = [Account(row) for row in rows]
                logger.info(f"ğŸ“š å·²ä»æ•°æ®åº“åŠ è½½ {len(self.accounts)} ä¸ªè´¦å·")
            else:
                logger.info("âš ï¸ æ•°æ®åº“ä¸­æ— å¯ç”¨è´¦å·ï¼Œå°è¯•ä½¿ç”¨ç¯å¢ƒå˜é‡ fallback")
                await self._load_fallback()
        except Exception as e:
            logger.error(f"âŒ åŠ è½½è´¦å·å¤±è´¥: {e}")
            await self._load_fallback()

    async def _load_fallback(self):
        if all([ENV_SECURE_C_SES, ENV_CSESIDX, ENV_CONFIG_ID]):
            fallback_data = {
                "id": 0,
                "name": "Env-Fallback",
                "secure_c_ses": ENV_SECURE_C_SES,
                "host_c_oses": ENV_HOST_C_OSES,
                "csesidx": ENV_CSESIDX,
                "config_id": ENV_CONFIG_ID
            }
            self.accounts = [Account(fallback_data)]
            logger.info("âœ… å·²åŠ è½½ç¯å¢ƒå˜é‡ fallback è´¦å·")
        else:
            logger.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•å¯ç”¨è´¦å·é…ç½®")

    async def get_next_account(self) -> Optional[Account]:
        """Failover Mode: Always return the first active account."""
        async with self._lock:
            if not self.accounts: return None

            # Always pick the FIRST active account (Primary)
            # This ensures stickiness unless the primary account fails/is disabled.
            # No Round-Robin rotation.
            for acc in self.accounts:
                if acc.id > 0: # Check real active flag if needed, but assuming self.accounts list is filtered/managed
                     # Actually self.accounts contains all. We rely on is_active in DB, but here we only have loaded accounts
                     pass
            
            # Simple approach: Return the first account. 
            # In a real failover system, we might want to check health, but here we just return index 0
            if self.accounts:
                 # Update usage stats for the primary account
                 if self.accounts[0].id > 0:
                     asyncio.create_task(db.update_account_usage(self.accounts[0].id))
                 return self.accounts[0]
            
            return None

    def get_account_by_id(self, account_id: int) -> Optional[Account]:
        for acc in self.accounts:
            if acc.id == account_id:
                return acc
        return None

account_pool = AccountPool()

# å…¨å±€ Session ç¼“å­˜ (Extended)
# Key: conv_key
# Value: {"session_id": str, "account_id": int, "updated_at": float}
SESSION_CACHE: Dict[str, dict] = {}

# ç”¨æˆ·æ¨¡å‹åå¥½ç¼“å­˜ (Model Stickiness)
# Key: client_ip
# Value: last_stream_model_name
USER_MODEL_PREF: Dict[str, str] = {}

async def create_google_session(account: Account) -> str:
    jwt = await account.get_jwt()
    headers = get_common_headers(jwt)
    body = {
        "configId": account.config_id,
        "additionalParams": {"token": "-"},
        "createSessionRequest": {
            "session": {"name": "", "displayName": ""}
        }
    }
    
    logger.debug(f"ğŸŒ [{account.name}] ç”³è¯·æ–° Session...")
    r = await http_client.post(
        "https://biz-discoveryengine.googleapis.com/v1alpha/locations/global/widgetCreateSession",
        headers=headers,
        json=body,
    )
    if r.status_code != 200:
        logger.error(f"âŒ createSession å¤±è´¥: {r.status_code} {r.text}")
        raise HTTPException(r.status_code, "createSession failed")
    sess_name = r.json()["session"]["name"]
    return sess_name

async def upload_context_file(account: Account, session_name: str, mime_type: str, base64_content: str) -> str:
    """ä¸Šä¼ æ–‡ä»¶åˆ°æŒ‡å®š Sessionï¼Œè¿”å› fileId"""
    jwt = await account.get_jwt()
    headers = get_common_headers(jwt)
    
    # ç”Ÿæˆéšæœºæ–‡ä»¶å
    ext = mime_type.split('/')[-1] if '/' in mime_type else "bin"
    file_name = f"upload_{int(time.time())}_{uuid.uuid4().hex[:6]}.{ext}"

    body = {
        "configId": account.config_id,
        "additionalParams": {"token": "-"},
        "addContextFileRequest": {
            "name": session_name,
            "fileName": file_name,
            "mimeType": mime_type,
            "fileContents": base64_content
        }
    }

    logger.info(f"ğŸ“¤ [{account.name}] ä¸Šä¼ å›¾ç‰‡ [{mime_type}] åˆ° Session...")
    r = await http_client.post(
        "https://biz-discoveryengine.googleapis.com/v1alpha/locations/global/widgetAddContextFile",
        headers=headers,
        json=body,
    )

    if r.status_code != 200:
        logger.error(f"âŒ ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {r.status_code} {r.text}")
        raise HTTPException(r.status_code, f"Upload failed: {r.text}")
    
    data = r.json()
    file_id = data.get("addContextFileResponse", {}).get("fileId")
    logger.info(f"âœ… å›¾ç‰‡ä¸Šä¼ æˆåŠŸ, ID: {file_id}")
    return file_id

# ---------- API Key éªŒè¯ ----------
async def verify_api_key(request: Request) -> None:
    if API_KEY is None: return
    auth_header = request.headers.get("authorization")
    if not auth_header or not auth_header.startswith("Bearer ") or auth_header[7:] != API_KEY:
        raise HTTPException(status_code=401, detail="Invalid API key")

# ---------- æ¶ˆæ¯å¤„ç†é€»è¾‘ ----------
def get_conversation_key(messages: List[dict]) -> str:
    if not messages: return "empty"
    # ä»…ä½¿ç”¨ç¬¬ä¸€æ¡æ¶ˆæ¯çš„å†…å®¹ç”ŸæˆæŒ‡çº¹ï¼Œå¿½ç•¥å›¾ç‰‡æ•°æ®é˜²æ­¢æŒ‡çº¹è¿‡å¤§
    first_msg = messages[0].copy()
    if isinstance(first_msg.get("content"), list):
        # å¦‚æœç¬¬ä¸€æ¡æ˜¯å¤šæ¨¡æ€ï¼Œåªå–æ–‡æœ¬éƒ¨åˆ†åš Hash
        text_part = "".join([x["text"] for x in first_msg["content"] if x["type"] == "text"])
        first_msg["content"] = text_part
    
    key_str = json.dumps(first_msg, sort_keys=True)
    return hashlib.md5(key_str.encode()).hexdigest()

def parse_last_message(messages: List['Message']):
    """è§£ææœ€åä¸€æ¡æ¶ˆæ¯ï¼Œåˆ†ç¦»æ–‡æœ¬å’Œå›¾ç‰‡"""
    if not messages:
        return "", []
    
    last_msg = messages[-1]
    content = last_msg.content
    
    text_content = ""
    images = [] # List of {"mime": str, "data": str_base64}

    if isinstance(content, str):
        text_content = content
    elif isinstance(content, list):
        for part in content:
            if part.get("type") == "text":
                text_content += part.get("text", "")
            elif part.get("type") == "image_url":
                url = part.get("image_url", {}).get("url", "")
                # è§£æ Data URI: data:image/png;base64,xxxxxx
                match = re.match(r"data:(image/[^;]+);base64,(.+)", url)
                if match:
                    images.append({"mime": match.group(1), "data": match.group(2)})
                else:
                    logger.warning(f"âš ï¸ æš‚ä¸æ”¯æŒé Base64 å›¾ç‰‡é“¾æ¥: {url[:30]}...")

    return text_content, images

def build_full_context_text(messages: List['Message']) -> str:
    """ä»…æ‹¼æ¥å†å²æ–‡æœ¬ï¼Œå›¾ç‰‡åªå¤„ç†å½“æ¬¡è¯·æ±‚çš„ã€‚å…¼å®¹å¤„ç† Tool Messagesã€‚"""
    prompt = ""
    for msg in messages:
        role = msg.role
        if role in ["user", "system"]:
            role_name = "User"
        elif role == "assistant":
            role_name = "Assistant"
        elif role == "tool":
            role_name = "Tool Output"
        else:
            role_name = "User" # Fallback

        content_str = ""
        if msg.content:
            if isinstance(msg.content, str):
                content_str = msg.content
            elif isinstance(msg.content, list):
                for part in msg.content:
                    if part.get("type") == "text":
                        content_str += part.get("text", "")
                    elif part.get("type") == "image_url":
                        content_str += "[å›¾ç‰‡]"
        
        # Helper for tool calls in assistant message
        if msg.tool_calls:
            for tc in msg.tool_calls:
                func_name = tc.get("function", {}).get("name", "unknown")
                args = tc.get("function", {}).get("arguments", "{}")
                content_str += f"\n[Call Tool: {func_name}({args})]"

        prompt += f"{role_name}: {content_str}\n\n"
    return prompt

# ---------- FastAPI App & Lifespan ----------
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await account_pool.load_accounts()
    yield
    # Shutdown
    await db.disconnect()

# ---------- OpenAI å…¼å®¹æ¥å£ ----------
app = FastAPI(title="Gemini-Business OpenAI Gateway", lifespan=lifespan)
# Mount static files for Admin UI
app.mount("/admin", StaticFiles(directory="static/admin", html=True), name="static")

# Admin API Models
class AccountCreate(BaseModel):
    name: str = "New Account"
    secure_c_ses: str
    host_c_oses: Optional[str] = None
    csesidx: str
    config_id: str

class AccountUpdate(BaseModel):
    name: Optional[str] = None
    secure_c_ses: Optional[str] = None
    host_c_oses: Optional[str] = None
    csesidx: Optional[str] = None
    config_id: Optional[str] = None
    is_active: Optional[bool] = None

# Admin API Routes
from fastapi import Depends

@app.get("/api/admin/accounts", dependencies=[Depends(verify_api_key)])
async def admin_list_accounts():
    return await db.get_all_accounts()

@app.post("/api/admin/accounts", dependencies=[Depends(verify_api_key)])
async def admin_add_account(acc: AccountCreate):
    await db.add_account(
        name=acc.name,
        secure_c_ses=acc.secure_c_ses,
        host_c_oses=acc.host_c_oses,
        csesidx=acc.csesidx,
        config_id=acc.config_id
    )
    await account_pool.load_accounts() # Refresh pool
    return {"status": "ok"}

@app.put("/api/admin/accounts/{id}", dependencies=[Depends(verify_api_key)])
async def admin_update_account(id: int, acc: AccountUpdate):
    data = acc.dict(exclude_unset=True)
    if not data: return {"status": "no change"}
    await db.update_account(id, data)
    await account_pool.load_accounts() # Refresh pool
    return {"status": "ok"}

@app.delete("/api/admin/accounts/{id}", dependencies=[Depends(verify_api_key)])
async def admin_delete_account(id: int):
    await db.delete_account(id)
    await account_pool.load_accounts() # Refresh pool
    return {"status": "ok"}


class Message(BaseModel):
    role: str
    content: Union[str, List[Dict[str, Any]], None] = None
    name: Optional[str] = None
    tool_calls: Optional[List[Dict[str, Any]]] = None
    tool_call_id: Optional[str] = None

class ChatRequest(BaseModel):
    model: str = "gemini-auto"
    messages: List[Message]
    stream: bool = False
    temperature: Optional[float] = 0.7
    top_p: Optional[float] = 1.0
    # OpenAI Compatibility Fields (Optional)
    tools: Optional[List[Dict[str, Any]]] = None
    tool_choice: Optional[Union[str, Dict[str, Any]]] = None
    max_tokens: Optional[int] = None
    n: Optional[int] = 1
    presence_penalty: Optional[float] = 0
    frequency_penalty: Optional[float] = 0
    stop: Optional[Union[str, List[str]]] = None

def create_chunk(id: str, created: int, model: str, delta: dict, finish_reason: Union[str, None]) -> str:
    chunk = {
        "id": id,
        "object": "chat.completion.chunk",
        "created": created,
        "model": model,
        "choices": [{
            "index": 0,
            "delta": delta,
            "finish_reason": finish_reason
        }]
    }
    return json.dumps(chunk)

@app.get("/v1/models")
async def list_models(request: Request):
    await verify_api_key(request)
    data = []
    now = int(time.time())
    for m in MODEL_MAPPING.keys():
        data.append({
            "id": m,
            "object": "model",
            "created": now,
            "owned_by": "google",
            "permission": []
        })
    return {"object": "list", "data": data}

@app.get("/health")
async def health():
    return {
        "status": "ok", 
        "time": datetime.utcnow().isoformat(),
        "accounts_loaded": len(account_pool.accounts)
    }

@app.post("/v1/chat/completions")
async def chat(req: ChatRequest, request: Request):
    await verify_api_key(request)
    # 1. æ¨¡å‹æ ¡éªŒ
    # æ¨æ–­è¯·æ±‚æ„å›¾ (Intent Inference)
    intent = "ğŸ’¬ ä¸»åŠ¨å¯¹è¯ (Chat)" if req.stream else "ğŸ¤– åå°ä»»åŠ¡ (Background/Title)"
    
    # DEBUG: Log raw received model with Intent
    logger.info(f"ğŸ“¨ è¯·æ±‚æ”¶åˆ° | ç±»å‹: {intent} | æ¨¡å‹: [{req.model}] | æµå¼: {req.stream}")
    
    # --- æ¨¡å‹ç²˜æ€§ä¸ä¸€è‡´æ€§ç­–ç•¥ (Model Stickiness) ---
    # ç­–ç•¥ï¼šä»¥æµå¼è¯·æ±‚(Stream=True)ä¸ºå‡†ï¼Œå› ä¸ºé‚£æ˜¯ç”¨æˆ·æ­£åœ¨è¿›è¡Œçš„çœŸå®å¯¹è¯ã€‚
    # éæµå¼(Stream=False)é€šå¸¸æ˜¯åå°ä»»åŠ¡(å¦‚æ ‡é¢˜ç”Ÿæˆ/æ‘˜è¦)ï¼Œå¾€å¾€ä½¿ç”¨é™çº§æ¨¡å‹(2.5)ã€‚
    # æˆ‘ä»¬è®°å½•ç”¨æˆ·æœ€åä¸€æ¬¡æµå¼è¯·æ±‚ä½¿ç”¨çš„æ¨¡å‹ï¼Œå¹¶å¼ºåˆ¶åç»­çš„éæµå¼è¯·æ±‚ä¿æŒä¸€è‡´ã€‚
    
    client_ip = request.client.host if request.client else "global"
    
    if req.stream:
        # ç”¨æˆ·æ˜¾å¼å‘èµ·å¯¹è¯ -> æ›´æ–°é¦–é€‰æ¨¡å‹è®°å½•
        USER_MODEL_PREF[client_ip] = req.model
    else:
        # åå°ä»»åŠ¡ -> æ£€æŸ¥æ˜¯å¦é€šè¿‡
        preferred = USER_MODEL_PREF.get(client_ip)
        if preferred and req.model != preferred:
             # å¦‚æœåå°è¯·æ±‚çš„æ¨¡å‹(å¦‚2.5)ä¸ç”¨æˆ·é¦–é€‰(å¦‚3)ä¸ä¸€è‡´ï¼Œå¼ºåˆ¶å‡çº§
             # ç‰¹ä¾‹ï¼šå¦‚æœç”¨æˆ·çœŸçš„æƒ³ç”¨2.5å‘éæµå¼ï¼Œè¿™é‡Œä¼šè¢«è¯¯ä¼¤ï¼Œä½†æƒè¡¡ä¹‹ä¸‹ï¼Œä¸€è‡´æ€§ä¼˜å…ˆ
             if "gemini-2.5" in req.model and "gemini-3" in preferred:
                 logger.info(f"âœ¨ [è‡ªåŠ¨å‡çº§] æ£€æµ‹åˆ°åå°é™çº§è¯·æ±‚ ({req.model}) -> å·²è‡ªåŠ¨ä¿®æ­£ä¸ºç”¨æˆ·é¦–é€‰ ({preferred})")
                 req.model = preferred

    if req.model not in MODEL_MAPPING:
        # Auto-map common aliases if needed, but for now strict check
        raise HTTPException(status_code=404, detail=f"Model '{req.model}' not found.")

    # 1.1 Compatibility Warning
    if req.tools:
        logger.warning(f"âš ï¸ å·¥å…·è°ƒç”¨è¢«å¿½ç•¥: ä¸Šæ¸¸ Gemini Widget æ¥å£æš‚ä¸æ”¯æŒ Client-Side Tools")

    # 2. è§£æè¯·æ±‚å†…å®¹
    last_text, current_images = parse_last_message(req.messages)
    
    # 3. é”šå®š Session
    # Fix Pydantic V2 deprecation warning
    conv_key = get_conversation_key([m.model_dump() for m in req.messages])
    cached = SESSION_CACHE.get(conv_key)
    
    account: Optional[Account] = None
    google_session: str = ""
    is_retry_mode = False

    # 3.1 å°è¯•ä»ç¼“å­˜æ¢å¤
    if cached:
        cached_acc_id = cached.get("account_id", 0)
        account = account_pool.get_account_by_id(cached_acc_id)
        
        # å¦‚æœç¼“å­˜çš„è´¦å·æ‰¾ä¸åˆ°äº†ï¼ˆæ¯”å¦‚è¢«ç¦ç”¨ï¼‰ï¼Œåˆ™éœ€è¦é‡æ–°å¼€å¯æ–°ä¼šè¯
        if account:
            google_session = cached["session_id"]
            text_to_send = last_text
            logger.info(f"â™»ï¸ å»¶ç»­æ—§å¯¹è¯ [{req.model}][Acc:{account.id}]: {google_session[-12:]}")
            SESSION_CACHE[conv_key]["updated_at"] = time.time()
        else:
            logger.warning(f"âš ï¸ ç¼“å­˜è´¦å· ID {cached_acc_id} ä¸å¯ç”¨ï¼Œå¼ºåˆ¶å¼€å¯æ–°å¯¹è¯")
            cached = None # Treat as new

    # 3.2 å¼€å¯æ–°ä¼šè¯ (å¦‚æœéœ€è¦)
    if not cached:
        account = await account_pool.get_next_account()
        if not account:
            raise HTTPException(status_code=503, detail="No active accounts available")
        
        logger.info(f"ğŸ›¡ï¸ [Primary/Sticky] Using Account: [{account.id}] {account.name}")
        logger.info(f"ğŸ†• å¼€å¯æ–°å¯¹è¯ [{req.model}][Acc:{account.id}]")
        try:
            google_session = await create_google_session(account)
            # æ–°å¯¹è¯ä½¿ç”¨å…¨é‡æ–‡æœ¬ä¸Šä¸‹æ–‡ (å›¾ç‰‡åªä¼ å½“å‰çš„)
            text_to_send = build_full_context_text(req.messages)
            SESSION_CACHE[conv_key] = {
                "session_id": google_session, 
                "account_id": account.id,
                "updated_at": time.time()
            }
            is_retry_mode = True
        except Exception as e:
            logger.error(f"âŒ å¼€å¯ä¼šè¯å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to create session: {e}")

    chat_id = f"chatcmpl-{uuid.uuid4()}"
    created_time = int(time.time())

    # å°è£…ç”Ÿæˆå™¨ (å«å›¾ç‰‡ä¸Šä¼ å’Œé‡è¯•é€»è¾‘)
    async def response_wrapper():
        retry_count = 0
        max_retries = 2
        
        current_text = text_to_send
        current_retry_mode = is_retry_mode
        
        # Important: Capture mutable variables for retry logic
        current_sess = google_session
        current_acc = account
        current_file_ids = []

        while retry_count <= max_retries:
            try:
                # A. å¦‚æœæœ‰å›¾ç‰‡ä¸”è¿˜æ²¡ä¸Šä¼ åˆ°å½“å‰ Sessionï¼Œå…ˆä¸Šä¼ 
                if current_images and not current_file_ids:
                    for img in current_images:
                        fid = await upload_context_file(current_acc, current_sess, img["mime"], img["data"])
                        current_file_ids.append(fid)

                # B. å‡†å¤‡æ–‡æœ¬ (é‡è¯•æ¨¡å¼ä¸‹å‘å…¨æ–‡)
                if current_retry_mode:
                    current_text = build_full_context_text(req.messages)

                # C. å‘èµ·å¯¹è¯
                async for chunk in stream_chat_generator(
                    current_acc,
                    current_sess, 
                    current_text, 
                    current_file_ids, 
                    req.model, 
                    chat_id, 
                    created_time, 
                    req.stream
                ):
                    yield chunk
                break 

            except (httpx.ConnectError, httpx.ReadTimeout, ssl.SSLError, HTTPException) as e:
                retry_count += 1
                logger.warning(f"âš ï¸ è¯·æ±‚å¼‚å¸¸ (é‡è¯• {retry_count}/{max_retries}): {e}")

                if retry_count <= max_retries:
                    # å°è¯•é‡å»º Session (ä»ä½¿ç”¨å½“å‰è´¦å·)
                    logger.info("ğŸ”„ å°è¯•é‡å»º Session...")
                    try:
                        new_sess = await create_google_session(current_acc)
                        if conv_key in SESSION_CACHE:
                            SESSION_CACHE[conv_key]["session_id"] = new_sess
                            # account_id keeps same
                        
                        current_sess = new_sess
                        current_retry_mode = True 
                        current_file_ids = [] 
                    except Exception as create_err:
                        logger.error(f"âŒ é‡å»ºå¤±è´¥: {create_err}")
                        if req.stream: yield f"data: {json.dumps({'error': {'message': 'Session Recovery Failed'}})}\n\n"
                        return
                else:
                    if req.stream: yield f"data: {json.dumps({'error': {'message': f'Final Error: {e}'}})}\n\n"
                    return

    if req.stream:
        return StreamingResponse(response_wrapper(), media_type="text/event-stream")
    
    full_content = ""
    async for chunk_str in response_wrapper():
        if chunk_str.startswith("data: [DONE]"): break
        if chunk_str.startswith("data: "):
            try:
                data = json.loads(chunk_str[6:])
                delta = data["choices"][0]["delta"]
                if "content" in delta: full_content += delta["content"]
            except: pass

    return {
        "id": chat_id,
        "object": "chat.completion",
        "created": created_time,
        "model": req.model,
        "choices": [{"index": 0, "message": {"role": "assistant", "content": full_content}, "finish_reason": "stop"}],
        "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
    }

# ---------- JSON Stream Parser ----------
class JSONStreamParser:
    def __init__(self):
        self.buffer_list = [] # Optimization: Use list for O(1) appends
        self.brace_count = 0
        self.in_string = False
        self.escape = False
        self.started = False 

    def process_chunk(self, chunk: str) -> List[str]:
        results = []
        for char in chunk:
            if not self.started:
                if char == '{':
                    self.started = True
                    self.brace_count = 1
                    self.buffer_list = ["{"]
                continue
            
            self.buffer_list.append(char)
            
            if self.in_string:
                if self.escape:
                    self.escape = False
                elif char == '\\':
                    self.escape = True
                elif char == '"':
                    self.in_string = False
            else:
                if char == '"':
                    self.in_string = True
                elif char == '{':
                    self.brace_count += 1
                elif char == '}':
                    self.brace_count -= 1
                    if self.brace_count == 0:
                        results.append("".join(self.buffer_list))
                        self.buffer_list = []
                        self.started = False
        return results

async def stream_chat_generator(account: Account, session: str, text_content: str, file_ids: List[str], model_name: str, chat_id: str, created_time: int, is_stream: bool = True):
    jwt = await account.get_jwt()
    headers = get_common_headers(jwt)
    
    body = {
        "configId": account.config_id,
        "additionalParams": {"token": "-"},
        "streamAssistRequest": {
            "session": session,
            "query": {"parts": [{"text": text_content}]},
            "filter": "",
            "fileIds": file_ids, 
            "answerGenerationMode": "NORMAL",
            "toolsSpec": {
                "webGroundingSpec": {},
                "toolRegistry": "default_tool_registry",
                "imageGenerationSpec": {},
                "videoGenerationSpec": {}
            },
            "languageCode": "zh-CN",
            "userMetadata": {"timeZone": "Asia/Shanghai"},
            "assistSkippingMode": "REQUEST_ASSIST"
        }
    }

    target_model_id = MODEL_MAPPING.get(model_name)
    if target_model_id:
        body["streamAssistRequest"]["assistGenerationConfig"] = {
            "modelId": target_model_id
        }

    if is_stream:
        chunk = create_chunk(chat_id, created_time, model_name, {"role": "assistant"}, None)
        yield f"data: {chunk}\n\n"

    parser = JSONStreamParser()
    
    # Use incremental decoder to handle multi-byte characters split across chunks
    import codecs
    decoder = codecs.getincrementaldecoder("utf-8")(errors="replace")

    try:
        async with http_client.stream(
            "POST",
            "https://biz-discoveryengine.googleapis.com/v1alpha/locations/global/widgetStreamAssist",
            headers=headers,
            json=body,
        ) as response:
            if response.status_code != 200:
                await response.aread()
                raise HTTPException(status_code=response.status_code, detail=f"Upstream Error {response.text}")

            # Smoothness Optimization:
            # The upstream API might return multiple JSON objects in a single chunk or split them.
            # We want to yield as soon as we have a displayable character.
            has_started_responding = False
            
            async for chunk_bytes in response.aiter_bytes(chunk_size=1024): # Try smaller chunks
                # Decode bytes incrementally
                chunk_str = decoder.decode(chunk_bytes, final=False)
                if not chunk_str:
                    continue
                    
                json_objects = parser.process_chunk(chunk_str)
                
                for json_str in json_objects:
                    try:
                        data = json.loads(json_str)
                        # Process the data immediately
                        for reply in data.get("streamAssistResponse", {}).get("answer", {}).get("replies", []):
                            content_obj = reply.get("groundedContent", {}).get("content", {})
                            text = content_obj.get("text", "")
                            
                            is_thought = reply.get("thought", False)
                            
                            # Optimized Filter Logic:
                            # If we haven't started responding yet (first token), we aggressively hide thoughts.
                            # Once valid text appears, we let everything through for speed.
                            
                            if not has_started_responding:
                                if text:
                                    clean_text = text.strip()
                                    # Very basic check for thought markers
                                    if clean_text.startswith("**") and clean_text.endswith("**") and len(clean_text) < 80:
                                        # Likely a thought header like "**Thought**"
                                        is_thought = True
                                    else:
                                        # Passed the filter
                                        has_started_responding = True
                            
                            # If filtered, log debug but don't yield (increases perceived latency but cleans output)
                            if is_thought and not has_started_responding:
                                logger.debug(f"ğŸ’­ Skipping thought: {text[:20]}...")
                                continue
                            
                            if text:
                                has_started_responding = True 
                                chunk = create_chunk(chat_id, created_time, model_name, {"content": text}, None)
                                if is_stream:
                                    yield f"data: {chunk}\n\n"
                                    # Anti-glitch: Small sleep 0 to force IO flush? 
                                    # Usually not needed in asyncio, but good for tight loops
                                    # await asyncio.sleep(0) 
                                else:
                                    pass
                    except json.JSONDecodeError:
                        logger.warning(f"âš ï¸ è§£æ JSON å¤±è´¥: {json_str[:50]}...")
                        continue

    except Exception as e:
        logger.error(f"âŒ æµå¼è¯·æ±‚å¼‚å¸¸: {e}")
        error_chunk = create_chunk(chat_id, created_time, model_name, {"content": f"\n[Error: {str(e)}]"}, "stop")
        if is_stream:
            yield f"data: {error_chunk}\n\n"
        raise e
    
    if is_stream:
        final_chunk = create_chunk(chat_id, created_time, model_name, {}, "stop")
        yield f"data: {final_chunk}\n\n"
        yield "data: [DONE]\n\n"

if __name__ == "__main__":
    if not (API_KEY):
        print("Error: Missing API_KEY variables.")
        exit(1)
    
    # Initialize Check
    if not (ENV_SECURE_C_SES or os.getenv("DATABASE_URL")):
         print("Warning: No Account Configs Found (ENV or DB).")

    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=7860)
